{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Executive Markdown\n",
    "================\n",
    "In this notebook I want to demonstrate a technique for detecting [lemmatisation](https://en.wikipedia.org/wiki/Lemmatisation) using a corpus of words from a target language.\n",
    "\n",
    "The idea: \n",
    "\n",
    "* There are $C$ concepts (the lemmas).\n",
    "* There are $T \\ge C$ words in the vocabulary.\n",
    "* A word belongs to 1 concept.\n",
    "* A word is a transmission of a concept using a sequence of symbols (letters) which are garbled at error rate $\\delta$ (spelling differences due to conjugations) and/or contain erasures (suffixes) at an erasure rate $\\epsilon$.\n",
    "* Group words into clusters using a weighted hamming metric to recover the lemmas.\n",
    "\n",
    "Problems to solve:\n",
    "\n",
    "1. Determine $C$ from the vocabulary.\n",
    "2. Determine the garble $\\delta$ and erasure $\\epsilon$ rates from the vocabulary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering and Error Correction\n",
    "\n",
    "I want to try to do error correction in the following way: for each word $x_i$ I want to \n",
    "\n",
    "1. compute the probability that word $x_j$ is in the same cluster as word $x_i$: \n",
    "$$p_{ij} = Pr(x_j \\in C_{x_i})$$\n",
    "2. compute the probability that letter $k$ of word $x_i$ is $c$, \n",
    "$$Pr(x_{i,k} = c| \\{x_j\\})$$ given the probabilities $(p_{ij})$ and the corresponding words $(x_j)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compute the first, I'll use Bayes' rule:\n",
    "$$Pr_{post}(A) = Pr(A|B) =  \\frac{Pr(A,B)}{Pr(B)} = \\frac{ Pr(B|A)Pr_{prior}(A) }{Pr(B)}$$\n",
    "\n",
    "When deciding that $x_j$ is in the same cluster as $x_i$ there are 2 opposing hypotheses to consider:\n",
    "\n",
    "1. non-causal random hypothesis: word $x_j$ is not in the same cluster as word $x_i$; there is no correlation between the letters of $x_j$ and those of $x_i$.\n",
    "2. causal hypothesis: word $x_j$ is in the same cluster as $x_i$; the letters of $x_j$ are correlated to those of $x_i$.\n",
    "\n",
    "In the first hypothesis the prior probability is $(C-1)/C$ and for the second hypothesis it's $1/C$ since we're assuming the number of members of each cluster is approximately the same, and so cluster membership should be a uniform distribution. The odds for a second ball to fall into the same cluster bin as the first ball is only $1/C$ vs. $(C-1)/C$ falling into another bin."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now I want to compute the ratio of the probabilities of the causal and random models given the observed $x_j$.\n",
    "\n",
    "I have to be able to compute the probability of the similarity between $x_i$ and $x_j$.\n",
    "Let $p_{l}$ denote the probability of letter $l$ occurring given the observed vocabulary.\n",
    "Then the probability of corresponding letters matching in the random case are:\n",
    "\n",
    "$$p_{random} := Pr(match) = \\sum_l {p^2}_l$$\n",
    "\n",
    "while in the causal case the probability of matching is\n",
    "\n",
    "$$p_{causal} := (1-\\delta)^2 + \\delta^2 p_{random}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When comparing words $x_i$ and $x_j$ we define:\n",
    "\n",
    "* $N := \\min(|x_i|,|x_j| )$\n",
    "* $A := \\{t | x_{i,t} = x_{j,t}, \\quad t \\le N \\}$\n",
    "* $D := \\{t | x_{i,t} \\ne x_{j,t}, \\quad t \\le N  \\}$\n",
    "* $E := \\{t | t>N\\}$\n",
    "* $q_{random} = 1- p_{random}$\n",
    "* $q_{causal} = 1 - p_{causal}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So to compute \n",
    "\\begin{align}\n",
    "L := \\frac{Pr(causal|x_j)}{Pr(random|x_j)} &= \\frac{ \\frac{Pr(x_j|causal)Pr_{prior}(causal)}{Pr(x_j)}}{ \\frac{Pr(x_j|random)Pr_{prior}(random)}{Pr(x_j)}} \\\\\n",
    "&= \\frac{ Pr(x_j|causal)Pr_{prior}(causal)}{ Pr(x_j|random)Pr_{prior}(random)} \\\\\n",
    "&= \\frac{ Pr(x_j|causal)}{Pr(x_j|random)}\\frac{1/C}{(C-1)/C}\\\\\n",
    "&= \\frac{\\binom{|A| + |D|}{|A|}p_{causal}^{|A|}q_{causal}^{|D|} }{\\binom{|A| + |D|}{|A|}p_{random}^{|A|}q_{random}^{|D|} }\\frac{1}{C-1}\\\\\n",
    "&= \\frac{p_{causal}^{|A|}q_{causal}^{|D|} }{p_{random}^{|A|}q_{random}^{|D|} }\\frac{1}{C-1} = \\frac{1}{C-1}\\left(\\frac{p_{causal}}{p_{random}}\\right)^{|A|}\\left(\\frac{q_{causal}}{q_{random}}\\right)^{|D|}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that I have a way to compute the ratio of probabilities $L$ of the 2 models given the observed word $x_j$ I can then compute the probability that $x_j$ is in the same cluster as $x_i$. Let $p_{i,j}$ denote the probability that $x_j$ is in the same cluster as $x_i$. Then we have:\n",
    "\n",
    "\\begin{align}\n",
    "L &= \\frac{p_{i,j}}{1-p_{i,j}} \\\\\n",
    "(1-p_{i,j})L &= p_{i,j} \\\\\n",
    "L &= (1+L)p_{i,j}\\\\\n",
    "p_{i,j} &= \\frac{L}{L+1}\\\\\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recovering the lemmas\n",
    "Once I have all the pairwise probabilities $\\{p_{i,j}\\}$ of belonging to the same cluster I have 2 strategies for recovering the clusters:\n",
    "\n",
    "1. Construct the graph of words with an edge connecting words $x_i, x_j$ if $p_{i,j} > cutoff$ for some $0 < cutoff<1$ value. Then find the connected components. Each such component is a cluster. The cutoff value can be determined by modeling the values $\\{\\log(p_{i,j})\\}$ as a mixture of 2 normal distributions, \n",
    "    * one for *pairs that are not in the same cluster* \n",
    "    * and the other for *pairs that are in the same cluster.*\n",
    "    \n",
    "2. Imposing an even stricter requirement on the graph, that all connected components need to be completely connected in order to be considered a cluster.\n",
    "\n",
    "After obtaining the clusters, for each cluster $C_k$ we choose the shortest length word $x_i$ which maximizes \n",
    "$$score = \\sum_{j \\in C_k} \\log\\left(p_{i,j}\\right)$$\n",
    "as the lemma for the cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recovering the erasure and garble rates from the data.\n",
    "Recovering the MLE erasure rate is relatively straight forward. The erasures in a vector can be modelled by a binomial distribution $(n,\\epsilon)$. One simply takes the sample mean of the observed erasures of the vectors to recover the erasure rate $\\epsilon$.\n",
    "\n",
    "To recover the garble rate $\\delta$ it suffices to model the hamming weight distribution as a mixture of 2 models: the random and the causal.\n",
    "\n",
    "Suppose we choose 2 words at random: $x_i,x_j$ and\n",
    "\n",
    "* we determine which coordinates are mutually free of erasures. Call this number $K$.\n",
    "* Count the number of coordinates in which they agree, $|A|$, and in which they disagree, $|D|$, with $|A| + |D| = K$.\n",
    "* Compute the mean hamming bit distance:\n",
    "$$hd_{i,j} = \\frac{|D|}{|A| + |D|}$$\n",
    "\n",
    "Can we determine the mean value $\\mu_{hd}$ for all $hd_{i,j}$? Yes, yes we can."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given 2 words $x_i,x_j$, after we've determined the erasure letters of $x_i, x_j$ we can model the remaining letters as being sampled from either \n",
    "\n",
    "* the random case with prior $(C-1)/C$ or \n",
    "* from the causal case with prior $1/C$. \n",
    "\n",
    "In the random case, the expected distribution for the hamming distance is independent of the garble rate while the expected distribution for the causal case will be a binomial distribution with parameters $(n = |A| +|D|,q)$ where $q_{causal} = 1 - p_{causal} = 1 - (1-\\delta)^2 - \\delta^2p_{random}$ which means \n",
    "\n",
    "* for the random case the mean hamming bit distance is $q_{random}$\n",
    "* for the causal case the mean hamming bit distance is $q_{causal}$\n",
    "\n",
    "So we can compute $\\mu_{hd}$ like so:\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{1}{T^2 -T}\\sum_{x_i \\ne x_j} hd_{i,j} &= \\mu_{hd}\\\\\n",
    "&= \\mu_{hd,random}Pr(random) + \\mu_{hd,causal}Pr(causal) \\\\\n",
    "&= \\mu_{hd,random}(C-1)/C + \\mu_{hd,causal}(1/C) \\\\\n",
    "&= q_{random}\\times (C-1)/C + q_{causal}\\times 1/C \\\\\n",
    "\\frac{1}{T^2 -T}\\sum_{x_i \\ne x_j} hd_{i,j} &= \\mu_{hd} = \\frac{q_{random}(C-1) + q_{causal}}{C}\n",
    "\\end{align}\n",
    "\n",
    "So we can recover the parameter $q_{causal}$ by computing the sample mean value of the hamming letter distance $\\mu_{hd}$:\n",
    "\n",
    "$$ q_{causal} = C\\mu_{hd} - (C-1)q_{random} $$\n",
    "Once we have $q_{causal}$ we can solve for $\\delta$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recovering $C$\n",
    "Recovering the estimate for $q_{causal}$ is predicated on knowledge of $C$. But we don't know $C$ yet. There are 2 possible techniques that I can think of for estimating $C$:\n",
    "\n",
    "1. Estimate $C$ by recovering the suffixes in the language and group words by stem. The number of stem groups should be $\\gtrapprox$ the number of lemma groups.\n",
    "\n",
    "2. Maximize the separation of groups via $\\{p_{ij}\\}$ like so:\n",
    "$$ score_{C} := \\sum_{i \\ne j} \\max(p_{ij}, 1- p_{ij}) $$\n",
    "For the correct $C$ we should have maximal separation of words into groups.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import string\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "try:\n",
    "    from bs4 import BeautifulSoup\n",
    "except ModuleNotFoundError:\n",
    "    # We don't have BeautifulSoup installed. We also need the lxml module\n",
    "    !pip install bs4 lxml\n",
    "    from bs4 import BeautifulSoup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pm_stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download Mody Dick as html and then extract the content\n",
    "!wget -c \"https://www.gutenberg.org/files/2701/2701-h/2701-h.htm\"\n",
    "htmltxt= open('2701-h.htm','r').read()\n",
    "soup = BeautifulSoup(htmltxt,'lxml')\n",
    "content = soup.text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content =\"\".join([x if x in string.ascii_letters else ' ' for x in content])\n",
    "words = word_tokenize(content)\n",
    "print(\"Length of word sequence: \",len(words))\n",
    "\n",
    "vocabulary = set(words)\n",
    "print(\"Size of vocabulary: \", len(vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary2 = list(vocabulary)\n",
    "vocabulary2.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = pm_stemmer.Stemmer()\n",
    "stemmer.fit(vocabulary2,verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "letter_prob = pm_stemmer.compute_alphabet_prob(vocabulary2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_random = sum([p_l**2 for p_l in letter_prob.values()])\n",
    "q_random = 1 - p_random\n",
    "print(\"Probability of letters pairing up at random: \", p_random)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_hd(vocabulary):\n",
    "    \"\"\"Compute the mean hadamard symbol error rate.\"\"\"\n",
    "    \n",
    "    # find the alphabet and translate it to ints so we can use numpy to speed things up.\n",
    "    max_word_size = max([len(word) for word in vocabulary])\n",
    "\n",
    "    alphabet = list(set([letter for letter in \"\".join(vocabulary)]))\n",
    "    alphabet_to_ints = dict([(x,t) for t,x in enumerate(alphabet)])\n",
    "    \n",
    "    vocab = [(len(word),word) for word in vocabulary ]\n",
    "    vocab.sort()\n",
    "    word_vecs = np.zeros((len(vocab),max_word_size),dtype=int)\n",
    "\n",
    "    for t,(l,word) in tqdm(enumerate(vocab),desc='generating word vectors'):\n",
    "        for i in range(l):\n",
    "            word_vecs[t,i] = alphabet_to_ints[word[i]]\n",
    "    \n",
    "    \n",
    "    agreements = 0\n",
    "    disagreements = 0\n",
    "    for t in tqdm(range(len(vocab)-1),desc='cross comparing'):\n",
    "        l,word = vocab[t]\n",
    "        compare_me = word_vecs[t+1:,:l]-word_vecs[t,:l].reshape(1,-1)\n",
    "        compare_me = (compare_me != 0).astype(int)\n",
    "        local_disagreements = compare_me.sum()\n",
    "        local_agreements = np.prod(compare_me.shape) - local_disagreements\n",
    "\n",
    "        agreements += local_agreements\n",
    "        disagreements += local_disagreements\n",
    "#    mean_hd = hd_sum*2/(len(vocab)*(len(vocab)-1))\n",
    "    mean_hd = disagreements/(agreements +disagreements)\n",
    "    return mean_hd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_hd = compute_hd(vocabulary2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_hd, q_random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_causal_probs = np.arange(C)*mean_hd - (np.arange(C)-1)*q_random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_causal, C_est  = q_causal_probs[q_causal_probs>0].min(),q_causal_probs[q_causal_probs>0].argmin()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C_stem = len(set(stemmer.stem_dict.values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm not happy on how I've arrived at $q_{causal}$ and $C$. Let's see if I can use them now to group words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_vocabulary(vocabulary):\n",
    "    \"\"\"Takes the list of words and sorts them by word length to make the\n",
    "    computations easier and faster. \"\"\"\n",
    "    \n",
    "    temp = [(len(word),word) for word in vocabulary]\n",
    "    temp.sort()\n",
    "    new_order = [x[1] for x in temp]\n",
    "    return new_order\n",
    "\n",
    "def compute_p_ij(vocabulary,q_random, q_causal,C):\n",
    "    \"\"\"Computes all possible pairings between vocabulary words.\n",
    "    returns the array p_ij \n",
    "    \n",
    "    CAVEAT: call this function with a sorted-by-word-length vocabulary\"\"\"\n",
    "    \n",
    "    p_causal = 1 - q_causal\n",
    "    p_random = 1 - q_random\n",
    "    \n",
    "    p_ratio = (p_causal/p_random)\n",
    "    q_ratio = (q_causal/q_random)\n",
    "    one_c = 1/(C-1)\n",
    "    \n",
    "    T = len(vocabulary)\n",
    "    max_word_size = len(vocabulary[-1])\n",
    "    \n",
    "    alphabet = list(set([letter for letter in \"\".join(vocabulary)]))\n",
    "    alphabet_to_ints = dict([(x,t) for t,x in enumerate(alphabet)])\n",
    "\n",
    "    vocab = [(len(word),word) for word in vocabulary]\n",
    "\n",
    "    word_vecs = np.zeros((T,max_word_size),dtype=int)\n",
    "    for t,(l,word) in tqdm(enumerate(vocab),desc='generating word vectors'):\n",
    "        for i in range(l):\n",
    "            word_vecs[t,i] = alphabet_to_ints[word[i]]\n",
    "\n",
    "    p_ij = np.zeros((T,T),dtype=np.float32)\n",
    "    \n",
    "    for t in tqdm(range(len(vocab)-1),desc='cross comparing'):\n",
    "        l,word = vocab[t]\n",
    "        # I don't want to bother if the vector is not at least 4 characters long.\n",
    "        if l < 5:\n",
    "            continue\n",
    "        compare_me = word_vecs[t+1:,:l]-word_vecs[t,:l].reshape(1,-1)\n",
    "        compare_me = (compare_me != 0).astype(int)\n",
    "        local_disagreements = compare_me.sum(axis=1)\n",
    "        local_agreements = l - local_disagreements\n",
    "        L = one_c*(p_ratio)**local_agreements*(q_ratio)**local_disagreements\n",
    "        p_ij[t,t+1:] = L/(L+1)\n",
    "        p_ij[t+1:,t] = L/(L+1)\n",
    "        \n",
    "    return p_ij"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary2 = sort_vocabulary(vocabulary2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_ij = compute_p_ij(vocabulary2,q_random, q_causal,C_stem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = p_ij.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist(temp[temp>0.1],bins=linspace(0.1,1,100));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(temp>0.8).sum()/temp.shape[0] * len(vocabulary2), (temp>0.8).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "possible = np.argwhere(p_ij>0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_names = [vocabulary2[a] for a,b in possible] + [vocabulary2[b] for a,b in possible]\n",
    "nodes = set(node_names)\n",
    "print(len(nodes), len(vocabulary2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.Graph([(vocabulary2[a],vocabulary2[b]) for (a,b) in possible])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = [x for x in nx.connected_components(G)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cliques =[x for x in nx.find_cliques(G)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bincount([len(x) for x in cliques])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "big_cliques = [x for x in cliques if len(x) == 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in big_cliques[0]:\n",
    "    print(x, stemmer.stem(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in cliques[:10]:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer.stem('constantly')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cliques[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(cliques)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.complete_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in range(100):\n",
    "    print(t, clusters[t])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for a,b in possible[:30]:\n",
    "    print(vocabulary2[a],vocabulary2[b])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for a,b in possible[-30:]:\n",
    "    print(vocabulary2[a],vocabulary2[b])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
