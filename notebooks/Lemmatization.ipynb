{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Executive Summary\n",
    "================\n",
    "In this notebook I want to demonstrate a technique for detecting [lemmatisation](https://en.wikipedia.org/wiki/Lemmatisation) using a corpus of words from a target language.\n",
    "\n",
    "Assumptions: \n",
    "\n",
    "* There are $C$ concepts (the lemmas).\n",
    "* There are $T \\ge C$ words in the vocabulary.\n",
    "* A word belongs to 1 concept.\n",
    "* A word is a transmission of a concept using a sequence of symbols (letters) which are garbled at error rate $\\delta$ (spelling differences due to conjugations) and/or contain erasures (suffixes) at an erasure rate $\\epsilon$.\n",
    "* Group words into clusters using a weighted hamming metric to recover the lemmas.\n",
    "* When words from the same concept are spelled differently, it's the vowels and not the consonants that change.\n",
    "\n",
    "Problems to solve:\n",
    "\n",
    "1. Determine $C$ from the vocabulary.\n",
    "2. Determine the vowels in the alphabet.\n",
    "2. Determine the garble $\\delta$ and erasure $\\epsilon$ rates from the vocabulary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering and Error Correction\n",
    "\n",
    "I want to try to do error correction in the following way: for each word $x_i$ I want to \n",
    "\n",
    "1. compute the probability that word $x_j$ is in the same cluster as word $x_i$: \n",
    "$$p_{ij} = Pr(x_j \\in C_{x_i})$$\n",
    "2. compute the probability that letter $k$ of word $x_i$ is $c$, \n",
    "$$Pr(x_{i,k} = c| \\{x_j\\})$$ given the probabilities $(p_{ij})$ and the corresponding words $(x_j)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compute the first, I'll use Bayes' rule:\n",
    "$$Pr_{post}(A) = Pr(A|B) =  \\frac{Pr(A,B)}{Pr(B)} = \\frac{ Pr(B|A)Pr_{prior}(A) }{Pr(B)}$$\n",
    "\n",
    "When deciding that $x_j$ is in the same cluster as $x_i$ there are 2 opposing hypotheses to consider:\n",
    "\n",
    "1. non-causal random hypothesis: word $x_j$ is not in the same cluster as word $x_i$; there is no correlation between the letters of $x_j$ and those of $x_i$.\n",
    "2. causal hypothesis: word $x_j$ is in the same cluster as $x_i$; the letters of $x_j$ are correlated to those of $x_i$.\n",
    "\n",
    "In the first hypothesis the prior probability is $(C-1)/C$ and for the second hypothesis it's $1/C$ since we're assuming the number of members of each cluster is approximately the same, and so cluster membership should be a uniform distribution. The odds for a second ball to fall into the same cluster bin as the first ball is only $1/C$ vs. $(C-1)/C$ falling into another bin."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now I want to compute the ratio of the probabilities of the causal and random models given the observed $x_j$.\n",
    "\n",
    "I have to be able to compute the probability of the similarity between $x_i$ and $x_j$.\n",
    "Let $p_{l}$ denote the probability of letter $l$ occurring given the observed vocabulary.\n",
    "Then the probability of corresponding letters matching in the random case are:\n",
    "\n",
    "$$p_{random} := Pr(match) = \\sum_l {p^2}_l$$\n",
    "\n",
    "while in the causal case the probability of matching is\n",
    "\n",
    "$$p_{causal} := (1-\\delta)^2 + \\delta^2 p_{random}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When comparing words $x_i$ and $x_j$ we define:\n",
    "\n",
    "* $N := \\min(|x_i|,|x_j| )$\n",
    "* $A := \\{t | x_{i,t} = x_{j,t}, \\quad t \\le N \\}$\n",
    "* $D := \\{t | x_{i,t} \\ne x_{j,t}, \\quad t \\le N  \\}$\n",
    "* $E := \\{t | t>N\\}$\n",
    "* $q_{random} = 1- p_{random}$\n",
    "* $q_{causal} = 1 - p_{causal}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So to compute \n",
    "\\begin{align}\n",
    "L := \\frac{Pr(causal|x_j)}{Pr(random|x_j)} &= \\frac{ \\frac{Pr(x_j|causal)Pr_{prior}(causal)}{Pr(x_j)}}{ \\frac{Pr(x_j|random)Pr_{prior}(random)}{Pr(x_j)}} \\\\\n",
    "&= \\frac{ Pr(x_j|causal)Pr_{prior}(causal)}{ Pr(x_j|random)Pr_{prior}(random)} \\\\\n",
    "&= \\frac{ Pr(x_j|causal)}{Pr(x_j|random)}\\frac{1/C}{(C-1)/C}\\\\\n",
    "&= \\frac{\\binom{|A| + |D|}{|A|}p_{causal}^{|A|}q_{causal}^{|D|} }{\\binom{|A| + |D|}{|A|}p_{random}^{|A|}q_{random}^{|D|} }\\frac{1}{C-1}\\\\\n",
    "&= \\frac{p_{causal}^{|A|}q_{causal}^{|D|} }{p_{random}^{|A|}q_{random}^{|D|} }\\frac{1}{C-1} = \\frac{1}{C-1}\\left(\\frac{p_{causal}}{p_{random}}\\right)^{|A|}\\left(\\frac{q_{causal}}{q_{random}}\\right)^{|D|}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that I have a way to compute the ratio of probabilities $L$ of the 2 models given the observed word $x_j$ I can then compute the probability that $x_j$ is in the same cluster as $x_i$. Let $p_{i,j}$ denote the probability that $x_j$ is in the same cluster as $x_i$. Then we have:\n",
    "\n",
    "\\begin{align}\n",
    "L &= \\frac{p_{i,j}}{1-p_{i,j}} \\\\\n",
    "(1-p_{i,j})L &= p_{i,j} \\\\\n",
    "L &= (1+L)p_{i,j}\\\\\n",
    "p_{i,j} &= \\frac{L}{L+1}\\\\\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recovering the lemmas\n",
    "Once I have all the pairwise probabilities $\\{p_{i,j}\\}$ of belonging to the same cluster I have 2 strategies for recovering the clusters:\n",
    "\n",
    "1. Construct the graph of words with an edge connecting words $x_i, x_j$ if $p_{i,j} > cutoff$ for some $0 < cutoff<1$ value. Then find the connected components. Each such component is a cluster. The cutoff value can be determined by modeling the values $\\{\\log(p_{i,j})\\}$ as a mixture of 2 normal distributions, \n",
    "    * one for *pairs that are not in the same cluster* \n",
    "    * and the other for *pairs that are in the same cluster.*\n",
    "    \n",
    "2. Imposing an even stricter requirement on the graph, that all connected components need to be completely connected in order to be considered a cluster.\n",
    "\n",
    "After obtaining the clusters, for each cluster $C_k$ we choose the shortest length word $x_i$ which maximizes \n",
    "$$score = \\sum_{j \\in C_k} \\log\\left(p_{i,j}\\right)$$\n",
    "as the lemma for the cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recovering the erasure and garble rates from the data.\n",
    "Recovering the MLE erasure rate is relatively straight forward. The erasures in a vector can be modelled by a binomial distribution $(n,\\epsilon)$. One simply takes the sample mean of the observed erasures of the vectors to recover the erasure rate $\\epsilon$.\n",
    "\n",
    "To recover the garble rate $\\delta$ it suffices to model the hamming weight distribution as a mixture of 2 models: the random and the causal.\n",
    "\n",
    "Suppose we choose 2 words at random: $x_i,x_j$ and\n",
    "\n",
    "* we determine which coordinates are mutually free of erasures. Call this number $K$.\n",
    "* Count the number of coordinates in which they agree, $|A|$, and in which they disagree, $|D|$, with $|A| + |D| = K$.\n",
    "* Compute the mean hamming bit distance:\n",
    "$$hd_{i,j} = \\frac{|D|}{|A| + |D|}$$\n",
    "\n",
    "Can we determine the mean value $\\mu_{hd}$ for all $hd_{i,j}$? Yes, yes we can."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given 2 words $x_i,x_j$, after we've determined the erasure letters of $x_i, x_j$ we can model the remaining letters as being sampled from either \n",
    "\n",
    "* the random case with prior $(C-1)/C$ or \n",
    "* from the causal case with prior $1/C$. \n",
    "\n",
    "In the random case, the expected distribution for the hamming distance is independent of the garble rate while the expected distribution for the causal case will be a binomial distribution with parameters $(n = |A| +|D|,q)$ where $q_{causal} = 1 - p_{causal} = 1 - (1-\\delta)^2 - \\delta^2p_{random}$ which means \n",
    "\n",
    "* for the random case the mean hamming bit distance is $q_{random}$\n",
    "* for the causal case the mean hamming bit distance is $q_{causal}$\n",
    "\n",
    "So we can compute $\\mu_{hd}$ like so:\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{1}{T^2 -T}\\sum_{x_i \\ne x_j} hd_{i,j} &= \\mu_{hd}\\\\\n",
    "&= \\mu_{hd,random}Pr(random) + \\mu_{hd,causal}Pr(causal) \\\\\n",
    "&= \\mu_{hd,random}(C-1)/C + \\mu_{hd,causal}(1/C) \\\\\n",
    "&= q_{random}\\times (C-1)/C + q_{causal}\\times 1/C \\\\\n",
    "\\frac{1}{T^2 -T}\\sum_{x_i \\ne x_j} hd_{i,j} &= \\mu_{hd} = \\frac{q_{random}(C-1) + q_{causal}}{C}\n",
    "\\end{align}\n",
    "\n",
    "So we can recover the parameter $q_{causal}$ by computing the sample mean value of the hamming letter distance $\\mu_{hd}$:\n",
    "\n",
    "$$ q_{causal} = C\\mu_{hd} - (C-1)q_{random} $$\n",
    "Once we have $q_{causal}$ we can solve for $\\delta$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recovering $C$\n",
    "Recovering the estimate for $q_{causal}$ is predicated on knowledge of $C$. But we don't know $C$ yet. There are 2 possible techniques that I can think of for estimating $C$:\n",
    "\n",
    "1. Estimate $C$ by recovering the suffixes in the language and group words by stem. The number of stem groups should be $\\gtrapprox$ the number of lemma groups.\n",
    "\n",
    "2. Maximize the separation of groups via $\\{p_{ij}\\}$ like so:\n",
    "$$ score_{C} := \\sum_{i \\ne j} \\max(p_{ij}, 1- p_{ij}) $$\n",
    "For the correct $C$ we should have maximal separation of words into groups.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recovering the vowels in the alphabet\n",
    "In languages that vowels are written and not assumed, we can try to recover the partition of letters into consonants and vowels by using an order 1 [hidden markov model](https://en.wikipedia.org/wiki/Hidden_Markov_model) (HMM) with 3 hidden states: whitespace, consonants, and vowels.\n",
    "\n",
    "The idea:\n",
    "\n",
    "1. There are $T$ words in the vocabulary.\n",
    "2. Each word $w_i$ in the vocabulary is a sequence of letters: $(w_{i,t})$, some of the letters are consonants, some are vowels.\n",
    "3. Concatenate using white space the vocabulary into  long sequence of words and spaces.\n",
    "2. For the Hidden Markov Model there are 3 parameters to recover: \n",
    "    * $\\pi_i :=Pr(X_0 = $consonant/vowel/white space$)$\n",
    "    * $A_{rs} := Pr(X_{t+1} = s | X_{t} = r)$\n",
    "    * $B_{r,s} := Pr(observed=s| X=r)$\n",
    "    \n",
    "Once we've recovered the optimal parameters $(\\pi_i),A,B$ we can recover the assignment of consonants and vowels as follows. There are 3 hidden states: one of them represents consonants, the other represents vowels, the last white space.\n",
    "\n",
    "$$B_{i,r} = Pr(\\text{ letter }r | \\text{hidden state }i)$$\n",
    "\n",
    "If we apply the $\\arg\\max{}$ function to the rows of $B$, $\\arg\\max_{i} (B_{i,r})$ we get a map $$\\arg\\max_{i}(B_{i,r}):r \\rightarrow \\text{hidden state } 0,1,2$$\n",
    "\n",
    "The white space hidden state has $B$ entry: $B(space) = 1$,  $B(letter) = 0$. The hidden state which has the most number of letters assigned to it are the consonants, and the other hidden state are the vowels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modifying the Expectation-Maximization algorithm for HMM\n",
    "In this [notebook](https://github.com/rpgomez/hidden_markov/blob/master/Hidden%20Markov%20Models.ipynb) I derive the steps for recovering a HMM from 1 observed sequence. We can modify the algorithm to handle multiple observed sequences or concatenate the vocabulary into 1 long stream interspersed with white space and 3 hidden states (consonant, vowel, white space) and\n",
    "with the $B$ matrix initialized with entries:\n",
    "\n",
    "1. $B_{consonant}(y) = \\begin{cases} random & y \\in alphabet \\\\ 0 & y = \\text{whitespace}\\end{cases}$\n",
    "2. $B_{vowel}(y) = \\begin{cases} random & y \\in alphabet \\\\ 0 & y = \\text{whitespace}\\end{cases}$\n",
    "3. $B_{whitespace} = \\begin{cases} 0 & y \\in alphabet \\\\ 1 & y = \\text{whitespace}\\end{cases}$\n",
    "\n",
    "the $\\pi$ array initialized with entries $[random, random,0]$\n",
    "\n",
    "Then when we recover the MLE $\\pi, A, B$ parameters we only focus on the first 2 rows of the $B$ matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import string\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "try:\n",
    "    from bs4 import BeautifulSoup\n",
    "except ModuleNotFoundError:\n",
    "    # We don't have BeautifulSoup installed. We also need the lxml module\n",
    "    !pip install bs4 lxml\n",
    "    from bs4 import BeautifulSoup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pm_stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download Mody Dick as html and then extract the content\n",
    "!wget -c \"https://www.gutenberg.org/files/2701/2701-h/2701-h.htm\"\n",
    "htmltxt= open('2701-h.htm','r').read()\n",
    "soup = BeautifulSoup(htmltxt,'lxml')\n",
    "content = soup.text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content =\"\".join([x if x in string.ascii_letters else ' ' for x in content])\n",
    "words = word_tokenize(content)\n",
    "print(\"Length of word sequence: \",len(words))\n",
    "\n",
    "vocabulary = set(words)\n",
    "print(\"Size of vocabulary: \", len(vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary2 = list(vocabulary)\n",
    "vocabulary2.sort()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use 2 white spaces to join words to force unbias between consonants and vowels and white space in a first order HMM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_string = \"  \".join(vocabulary2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphabet =  list(set([x for x in one_string]))\n",
    "alphabet.sort()\n",
    "print(alphabet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hmm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B = np.random.dirichlet(np.ones(len(alphabet)),3)\n",
    "\n",
    "B[2,:] = 0.0\n",
    "B[2,0] = 1\n",
    "B[:2,0] = 0.0\n",
    "B[:2] = B[:2]/B[:2].sum(axis=1).reshape(-1,1)\n",
    "\n",
    "my_pi = np.ones(3)*0.5\n",
    "my_pi[2] = 0.0 # Not in the white space state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_hmm = hmm.hmm(num_hiddenstates=3,B =B.copy(),pi = my_pi.copy() )\n",
    "my_hmm.fit(one_string[:100]) # to precompile hmm code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "my_hmm = hmm.hmm(num_hiddenstates=3,B =B.copy(),pi = my_pi.copy() )\n",
    "my_hmm.fit(one_string,verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title(\"Heat map of B matrix\")\n",
    "ylabel(\"Hidden State, 2 == white space\")\n",
    "xlabel(\"Observed letter, 0 == white space\")\n",
    "imshow(my_hmm.B,origin='lower',aspect='auto',interpolation='none'); colorbar();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_assignments = my_hmm.B.argmax(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = 1/3*my_hmm.B.max(axis=0).sum()\n",
    "print(\"Probability of clustering letters into correct groups: {0:6.4f}%\".format(100*accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = [[] for i in range(3)]\n",
    "for y in range(my_hmm.B.shape[1]):\n",
    "    letter = my_hmm.inv_map[y]\n",
    "    assigned = cluster_assignments[y]\n",
    "    clusters[assigned].append(letter)\n",
    "\n",
    "for i in range(3):\n",
    "    clusters[i].sort() \n",
    "    clusters[i] = \",\".join(clusters[i])\n",
    "    \n",
    "print(\"Clusters:\")\n",
    "print(\"\\n=====\\n\".join(clusters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(clusters[0]) < len(clusters[1]):\n",
    "    vowels = clusters[0]\n",
    "else:\n",
    "    vowels = clusters[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = pm_stemmer.Stemmer()\n",
    "stemmer.fit(vocabulary2,verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "letter_prob = pm_stemmer.compute_alphabet_prob(vocabulary2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_random = sum([p_l**2 for p_l in letter_prob.values()])\n",
    "q_random = 1 - p_random\n",
    "print(\"Probability of letters pairing up at random: \", p_random)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_hd(vocabulary):\n",
    "    \"\"\"Compute the mean hadamard symbol error rate.\"\"\"\n",
    "    \n",
    "    # find the alphabet and translate it to ints so we can use numpy to speed things up.\n",
    "    max_word_size = max([len(word) for word in vocabulary])\n",
    "\n",
    "    alphabet = list(set([letter for letter in \"\".join(vocabulary)]))\n",
    "    alphabet_to_ints = dict([(x,t) for t,x in enumerate(alphabet)])\n",
    "    \n",
    "    vocab = [(len(word),word) for word in vocabulary ]\n",
    "    vocab.sort()\n",
    "    word_vecs = np.zeros((len(vocab),max_word_size),dtype=int)\n",
    "\n",
    "    for t,(l,word) in tqdm(enumerate(vocab),desc='generating word vectors'):\n",
    "        for i in range(l):\n",
    "            word_vecs[t,i] = alphabet_to_ints[word[i]]\n",
    "    \n",
    "    \n",
    "    agreements = 0\n",
    "    disagreements = 0\n",
    "    for t in tqdm(range(len(vocab)-1),desc='cross comparing'):\n",
    "        l,word = vocab[t]\n",
    "        compare_me = word_vecs[t+1:,:l]-word_vecs[t,:l].reshape(1,-1)\n",
    "        compare_me = (compare_me != 0).astype(int)\n",
    "        local_disagreements = compare_me.sum()\n",
    "        local_agreements = np.prod(compare_me.shape) - local_disagreements\n",
    "\n",
    "        agreements += local_agreements\n",
    "        disagreements += local_disagreements\n",
    "#    mean_hd = hd_sum*2/(len(vocab)*(len(vocab)-1))\n",
    "    mean_hd = disagreements/(agreements +disagreements)\n",
    "    return mean_hd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_hd = compute_hd(vocabulary2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_hd, q_random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = len(stemmer.stem_dict)\n",
    "q_causal_probs = np.arange(C)*mean_hd - (np.arange(C)-1)*q_random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_causal, C_est  = q_causal_probs[q_causal_probs>0].min(),q_causal_probs[q_causal_probs>0].argmin()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C_stem = len(set(stemmer.stem_dict.values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm not happy on how I've arrived at $q_{causal}$ and $C$. Let's see if I can use them now to group words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_vocabulary(vocabulary):\n",
    "    \"\"\"Takes the list of words and sorts them by word length to make the\n",
    "    computations easier and faster. \"\"\"\n",
    "    \n",
    "    temp = [(len(word),word) for word in vocabulary]\n",
    "    temp.sort()\n",
    "    new_order = [x[1] for x in temp]\n",
    "    return new_order\n",
    "\n",
    "def compute_p_ij(vocabulary,q_random, q_causal,C):\n",
    "    \"\"\"Computes all possible pairings between vocabulary words.\n",
    "    returns the array p_ij \n",
    "    \n",
    "    CAVEAT: call this function with a sorted-by-word-length vocabulary\n",
    "    \n",
    "    I want to score as valid difference only those letter pairs\n",
    "    that differ by vowels, i.e., (swim,swam) are fine to compare,\n",
    "    but (abase,abate) are not.\n",
    "    \"\"\"\n",
    "    \n",
    "    p_causal = 1 - q_causal\n",
    "    p_random = 1 - q_random\n",
    "    \n",
    "    p_ratio = (p_causal/p_random)\n",
    "    q_ratio = (q_causal/q_random)\n",
    "    one_c = 1/(C-1)\n",
    "    \n",
    "    T = len(vocabulary)\n",
    "    max_word_size = len(vocabulary[-1])\n",
    "    \n",
    "    alphabet = list(set([letter for letter in \"\".join(vocabulary)]))\n",
    "    alphabet_to_ints = dict([(x,t) for t,x in enumerate(alphabet)])\n",
    "\n",
    "    vocab = [(len(word),word) for word in vocabulary]\n",
    "\n",
    "    word_vecs = np.zeros((T,max_word_size),dtype=int)\n",
    "    for t,(l,word) in tqdm(enumerate(vocab),desc='generating word vectors'):\n",
    "        for i in range(l):\n",
    "            word_vecs[t,i] = alphabet_to_ints[word[i]]\n",
    "\n",
    "    p_ij = np.zeros((T,T),dtype=np.float32)\n",
    "    \n",
    "    for t in tqdm(range(len(vocab)-1),desc='cross comparing'):\n",
    "        l,word = vocab[t]\n",
    "        # I don't want to bother if the vector is not at least 4 characters long.\n",
    "        if l < 5:\n",
    "            continue\n",
    "        compare_me = word_vecs[t+1:,:l]-word_vecs[t,:l].reshape(1,-1)\n",
    "        compare_me = (compare_me != 0).astype(int)\n",
    "        local_disagreements = compare_me.sum(axis=1)\n",
    "        local_agreements = l - local_disagreements\n",
    "        L = one_c*(p_ratio)**local_agreements*(q_ratio)**local_disagreements\n",
    "        p_ij[t,t+1:] = L/(L+1)\n",
    "        p_ij[t+1:,t] = L/(L+1)\n",
    "        \n",
    "    return p_ij"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_p_ij(p_ij, word1,word2,vowels):\n",
    "    \"\"\"Sets p_ij = 0 if the difference in word1 and word2 are (consonant, consonant)\n",
    "    or (consonant, vowel) pairs.\"\"\"\n",
    "    \n",
    "    lengths = [(len(word1),word1),(len(word2),word2)]\n",
    "    lengths.sort()\n",
    "    wor1,wor2 = lengths[0][1], lengths[1][1]\n",
    "    \n",
    "    for t in range(len(wor1)):\n",
    "        if wor1[t] != wor2[t]:\n",
    "            if not wor1[t] in vowels:\n",
    "                return 0.0\n",
    "            if not wor2[t] in vowels:\n",
    "                return 0.0\n",
    "            \n",
    "    return p_ij"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary2 = sort_vocabulary(vocabulary2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_ij = compute_p_ij(vocabulary2,q_random, q_causal,C_stem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "possible_pairing = np.argwhere(p_ij > 0.8)\n",
    "print(\"# of pairings to consider:\", len(possible_pairing))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (i,j) in tqdm(possible_pairing,desc='second filter'):\n",
    "    word1, word2 = vocabulary2[i], vocabulary2[j]\n",
    "    update_score = update_p_ij(p_ij[i,j],word1,word2,vowels)\n",
    "    p_ij[i,j] = update_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "possible_pairing = np.argwhere(p_ij > 0.8)\n",
    "print(\"# of pairings to consider:\", len(possible_pairing))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = p_ij.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist(temp[temp>0.1],bins=linspace(0.1,1,100));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(temp>0.8).sum()/temp.shape[0] * len(vocabulary2), (temp>0.8).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "possible = np.argwhere(p_ij>0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_names = [vocabulary2[a] for a,b in possible] + [vocabulary2[b] for a,b in possible]\n",
    "nodes = set(node_names)\n",
    "print(len(nodes), len(vocabulary2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.Graph([(vocabulary2[a],vocabulary2[b]) for (a,b) in possible])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = [x for x in nx.connected_components(G)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cliques =[x for x in nx.find_cliques(G)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bincount([len(x) for x in cliques])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "big_cliques = [x for x in cliques if len(x) == 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in big_cliques[0]:\n",
    "    print(x, stemmer.stem(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in cliques[:10]:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer.stem('constantly'), stemmer.stem('extravaganzas')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cliques[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in range(100):\n",
    "    print(\"{0:02}\".format(t), \" \\t\".join(list(clusters[t])[:5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for a,b in possible[:30]:\n",
    "    print(vocabulary2[a],vocabulary2[b])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for a,b in possible[-30:]:\n",
    "    print(vocabulary2[a],vocabulary2[b])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
