{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Executive Markdown\n",
    "================\n",
    "In this notebook I want to demonstrate a technique for detecting stems using a corpus of words from a target language.\n",
    "\n",
    "## Finding Suffixes\n",
    "The idea: find the prefixes/suffixes\n",
    "\n",
    "1. I have a vocabulary of words $vocab := \\{w_i \\}$. Let $|vocab|$ denote the size of the vocabulary.\n",
    "2. I have the alphabet $\\{a_j\\}$ used to construct the words.\n",
    "\n",
    "I have a putative suffix $(a_1,a_2,\\ldots, a_r)$. I would like to know if it's a suffix. How do I determine it's a suffix?\n",
    "\n",
    "1. Make a table of counts of the $r$ long *observed* tail sequences $\\{ (b_1,\\ldots,b_r): C_{(b_1,\\ldots,b_r)}\\}$ in the vocabulary of words. \n",
    "2. Let $T$ denote the number the distinct $r$ long observed sequences. Compute the probability of a sequence $(b_1,\\ldots,b_r)$ occurring $C_{(b_1,\\ldots,b_r)}$ times or more in the vocabulary space under the null hypothesis that each such possible sequence has probability $1/T$ of occurring:\n",
    "$$ p\\text{ value} := \\sum_{t= C_{(b_1,\\ldots,b_r)}}^{|vocab|}\\binom{|vocab|}{t} p^t(1-p)^{|vocab| - t},\\quad p:= 1/T $$\n",
    "\n",
    "Alternatively, make a frequency count of the letters in the vocabulary of words, and define a probability distribution on the space of letters, $(p_{a_j})$. Then define the probability of observed sequence $(b_1,\\ldots,b_r)$ occurring to be:\n",
    "\n",
    "$$ Pr((b_1,\\ldots,b_r)) = \\frac{\\prod_{i=1}^r p_{b_i}}{\\sum_{\\text{observed sequences }(d_1,\\ldots,d_r)}\\prod_{i=1}^r p_{d_i}}$$\n",
    "Then we compute the $p$ value: \n",
    "$$p\\text{ value} := \\sum_{t= C_{(b_1,\\ldots,b_r)}}^{|vocab|}\\binom{|vocab|}{t}Pr((b_1,\\ldots,b_r))^t(1-Pr((b_1,\\ldots,b_r)))^{|vocab| - t}$$\n",
    "\n",
    "If $p\\text{ value} < 0.01/T$ it's a suffix or a subset of one. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stems\n",
    "Once I have a collection of putative suffixes $\\{ sf_i \\}$ I would like to know what are putative stems $\\{st_n\\}$. Here are my assumptions:\n",
    "\n",
    "1. Let $V$ denote the number of words in the vocabulary $\\{ word_v\\}$.\n",
    "1. All stems are at least 4 characters long.\n",
    "2. Let $N$ be the number of all possible putative stems.\n",
    "3. Let $M$ be the number of words that contain a putative suffix.\n",
    "\n",
    "For each putative stem $st_n$ let $C_n$ denote the size of the set $\\{ word_k | word_k = st_n + sf_i \\text{ for some } i\\}$.\n",
    "\n",
    "* How can I determine that a putative stem $st_n$ is an actual stem?\n",
    "* Do stems have a preferred list of suffixes to use to form words?\n",
    "\n",
    "### Determining Confidence of Stems\n",
    "We can model the putative stems as bins and words with putative suffixes as balls. We can model $C_n$ as a Poisson distribution $X \\sim Poisson(\\lambda)$ with parameter $\\lambda = M/N$.\n",
    "\n",
    "Since there are $N$ putative stems, we're performing $N$ tests and therefore we can use a hypothesis test  with $p$ value cutoff $p = 0.01/N$.\n",
    "\n",
    "We can test $Pr(X >= C_n) < p$ to decide if putative stem $st_n$ is a stem.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determining Stem Groups\n",
    "I want to determine if suffix $sf_i \\sim sf_j$ in the sense that if $st_n$ is a stem and $st_n + sf_i$ is a word then so is (probably) $st_n + sf_j$. So how can I convince myself that $sf_i \\sim sf_j$? Co-occurence. If there exists a stem $st_n$ such that both $st_n + sf_i$ and $st_n + sf_j$ are words then we say $sf_i, sf_j$ have co-occurred.\n",
    "\n",
    "* Let $S$ denote the number of suffixes.\n",
    "* Let $M_i$ be the number of occurrences of suffix $sf_i$ in my vocabulary.\n",
    "* Let $N$ denote the number of stems.\n",
    "* Let $N_{ij}$ denote the number of co-occurences of suffix $sf_i$ and $sf_j$.\n",
    "\n",
    "What is the probability of $X=N_{ij}$? For each stem, the suffix $sf_i$ can only occur at most once. So the $M_i$ occurrences have to occur at $M_i$ slots. Ditto for $sf_j$. So\n",
    "the probability of $X=N_{ij}$ occurring is \n",
    "\n",
    "\\begin{align}\n",
    "Pr(X=N_{ij}) &= \\frac{\\binom{M_i}{N_{ij}}\\binom{N-M_i}{M_j -N_{ij}}}{\\binom{N}{M_j}}\n",
    "\\end{align}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So to decide  $sf_i \\sim sf_j$ we will perform $\\binom{S}{2}$ pair wise tests and\n",
    "decide $sf_i \\sim sf_j$ if $Pr(X>=N_{ij}) < 0.01/\\binom{S}{2}$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "import math\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import string\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "try:\n",
    "    from bs4 import BeautifulSoup\n",
    "except ModuleNotFoundError:\n",
    "    # We don't have BeautifulSoup installed. We also need the lxml module\n",
    "    !pip install bs4 lxml\n",
    "    from bs4 import BeautifulSoup\n",
    "\n",
    "try:\n",
    "    import networkx as nx\n",
    "except:\n",
    "    # We don't have networkx installed. We need networkx\n",
    "    !pip install networkx\n",
    "    import networkx as nx\n",
    "    \n",
    "try:\n",
    "    import graphviz\n",
    "except:\n",
    "    # We don't have graphviz installed. Let's install it.\n",
    "    !pip install graphviz\n",
    "    import graphviz\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pm_stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download Mody Dick as html and then extract the content\n",
    "!wget -c \"https://www.gutenberg.org/files/2701/2701-h/2701-h.htm\"\n",
    "htmltxt= open('2701-h.htm','r').read()\n",
    "soup = BeautifulSoup(htmltxt,'lxml')\n",
    "content = soup.text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content =\"\".join([x if x in string.ascii_letters else ' ' for x in content])\n",
    "words = word_tokenize(content)\n",
    "print(\"Length of word sequence: \",len(words))\n",
    "\n",
    "vocabulary = set(words)\n",
    "print(\"Size of vocabulary: \", len(vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_lgamma(x):\n",
    "    \"\"\"Computes log(x!)\n",
    "    \n",
    "    because math.lgamma(x) == log((x-1)!)\"\"\"\n",
    "    return math.lgamma(x+1)\n",
    "\n",
    "def pr_cooccurrence(n_ij, m_i,m_j, N):\n",
    "    \"\"\"Computes Pr(n_ij | m_i, m_j, N)\"\"\"\n",
    "    numerator = my_lgamma(m_i) - my_lgamma(n_ij) - my_lgamma(m_i-n_ij) + \\\n",
    "    my_lgamma(N- m_i) - my_lgamma(m_j - n_ij) - my_lgamma(N -m_i - (m_j - n_ij))\n",
    "    denominator = my_lgamma(N) - my_lgamma(m_j) - my_lgamma(N-m_j)\n",
    "    logratio = numerator - denominator\n",
    "    result = np.exp(logratio)\n",
    "    return result\n",
    "\n",
    "def compute_p_value(n_ij, m_i,m_j, N):\n",
    "    \"\"\"Computes Pr(X>=n_ij)\"\"\"\n",
    "    \n",
    "    m_j_temp, m_i_temp = min(m_i,m_j), max(m_i,m_j)\n",
    "    probs = sum([pr_cooccurrence(c,m_i_temp,m_j_temp,N) \\\n",
    "                     for c in range(n_ij, m_j_temp+1)])\n",
    "        \n",
    "    return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_stem_suffix_counts(vocabulary, suffixes):\n",
    "    \"\"\"Computes m_i, N, stem dictionaries\"\"\"\n",
    "    \n",
    "    vocab = [(len(word),word) for word in vocabulary]\n",
    "    vocab.sort()\n",
    "\n",
    "    stem_dictionary = defaultdict(list)\n",
    "    m_i = defaultdict(int)\n",
    "    \n",
    "    for l,aword in tqdm(vocab,desc='constructing stem dictionary'):\n",
    "        if l < 4: # if a word is not at least 4 letters long, it's not a stem for me.\n",
    "            continue\n",
    "            \n",
    "        for t in range(4,l+1):\n",
    "            stem = aword[:t]\n",
    "            suffix = aword[t:]\n",
    "            if suffix in suffixes:\n",
    "                stem_dictionary[stem].append(suffix)\n",
    "                m_i[suffix] += 1\n",
    "    \n",
    "    return m_i, stem_dictionary\n",
    "\n",
    "def compute_co_occurrence(stem_dictionary):\n",
    "    \"\"\"Computes n_ij for all suffixes i, j\"\"\"\n",
    "    cooccurrences = defaultdict(int)\n",
    "    for astem in tqdm(stem_dictionary,desc='constructing n_ij'):\n",
    "        hits = stem_dictionary[astem]\n",
    "        if len(hits) > 1:\n",
    "            # got a hit.\n",
    "            hits.sort()\n",
    "            for i,s_i in enumerate(hits[:-1]):\n",
    "                for j,s_j in enumerate(hits[i+1:]):\n",
    "                    cooccurrences[(s_i,s_j)] +=1\n",
    "                    \n",
    "    return cooccurrences    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_num_stems(vocabulary):\n",
    "    \"\"\"Computes all possible stems whether valid or not. \"\"\"\n",
    "    vocab = [(len(word),word) for word in vocabulary]\n",
    "    vocab.sort()\n",
    "\n",
    "    stems = set()\n",
    "    \n",
    "    for l,aword in tqdm(vocab,desc='constructing stem dictionary'):\n",
    "        if l < 4: # if a word is not at least 4 letters long, it's not a stem for me.\n",
    "            continue\n",
    "        stems = stems.union(set([aword[:t] for t in range(4,l+1)]))\n",
    "\n",
    "    return len(stems)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def determine_stems(stem_dictionary,num_stems, cutoff=0.01):\n",
    "    \"\"\"Takes the stem dictionary (stem, [suffixes])\n",
    "    and decides which putative stems are actually stems.\n",
    "    returns a list of stems that make the cutoff:\n",
    "    \n",
    "    Pr(X >= C_n) < p = cutoff/N \"\"\"\n",
    "    \n",
    "    M = sum([len(x) for x in stem_dictionary.values()])\n",
    "    N = num_stems\n",
    "    lam = M/N\n",
    "    p = cutoff/len(stem_dictionary) # The # of bins I'm actually testing.\n",
    "    C_n = {x:len(stem_dictionary[x]) for x in stem_dictionary}\n",
    "    print(M,N,lam,p)\n",
    "    model = scipy.stats.poisson(mu=lam)\n",
    "    scores = {x:model.sf(C_n[x]) for x in tqdm(C_n,desc='Computing Prob')}\n",
    "    survivors = [x for x in scores if scores[x]< p]\n",
    "    survivors.sort()\n",
    "    return survivors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary2 = list(vocabulary)\n",
    "vocabulary2.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = pm_stemmer.Stemmer()\n",
    "stemmer.fit(vocabulary2,verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "suffixes = []\n",
    "for l in stemmer.possible_suffixes:\n",
    "    suffixes += [suffix for suffix in stemmer.possible_suffixes[l]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_i, stem_dictionary = compute_stem_suffix_counts(vocabulary, suffixes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_stems = compute_num_stems(vocabulary2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stems = determine_stems(stem_dictionary,N_stems)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(stems, len(stem_dictionary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(stems)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cooccurrence = compute_co_occurrence(stem_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = len(stem_dictionary)\n",
    "co_occ_probs = {}\n",
    "for s_i,s_j in tqdm(cooccurrence,desc='Computing Pr(X=n_ij)'):\n",
    "    mi = m_i[s_i]\n",
    "    mj = m_i[s_j]\n",
    "    n_ij = cooccurrence[(s_i,s_j)]\n",
    "    co_occ_probs[(s_i,s_j)] = compute_p_value(n_ij, mi,mj, N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = np.array(list(co_occ_probs.values()))\n",
    "print(probs.min(),probs.mean(),probs.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = len(probs)\n",
    "cutoff = 0.01/T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mi_sim_mj = [x for x in co_occ_probs if co_occ_probs[x]< cutoff]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mi_sim_mj.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.Graph(mi_sim_mj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(mi_sim_mj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "components = [(len(x), x) for x in nx.components.connected_components(G)]\n",
    "components.sort()\n",
    "components.reverse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_sizes = np.argwhere(bincount([x[0] for x in components])>0).flatten()\n",
    "print(cluster_sizes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of equivalence classes: \", len(components))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_subset_1 = G.subgraph(components[1][1])\n",
    "\n",
    "H = graphviz.Graph()\n",
    "for an_edge in G_subset_1.edges():\n",
    "    H.edge(*an_edge)\n",
    "\n",
    "H.render('component1_suffixes.gv',format='png')\n",
    "H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_subset_2 = G.subgraph(components[2][1])\n",
    "\n",
    "H = graphviz.Graph()\n",
    "for an_edge in G_subset_2.edges():\n",
    "    H.edge(*an_edge)\n",
    "\n",
    "H.render('component2_suffixes.gv',format='png')\n",
    "H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_subset_6 = G.subgraph(components[6][1])\n",
    "\n",
    "H = graphviz.Graph()\n",
    "for an_edge in G_subset_6.edges():\n",
    "    H.edge(*an_edge)\n",
    "\n",
    "H.render('component6_suffixes.gv',format='png')\n",
    "H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_subset_1.edges()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_subset_1 = G.subgraph(components[1][1])\n",
    "figure(figsize=(16,16))\n",
    "nx.draw_networkx(G_subset_1,node_size = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_subset = G.subgraph(components[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure(figsize=(16,16))\n",
    "nx.draw_networkx(G_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure(figsize=(16,16))\n",
    "nx.draw(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist(probs,bins=linspace(probs.min(),probs.max(),100));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer.stem('following'), stemmer.stem('abandonedly')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fred = set(stem_map.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of stems, size of vocabulary, ratio: \", \\\n",
    "      len(fred),len(vocabulary2), len(vocabulary2)/len(fred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stem_counts = Counter(stem_map.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title(\"Distribution of stem cluster sizes\")\n",
    "hist(list(stem_counts.values()),bins=np.arange(stem_counts.most_common(1)[0][1]+1));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distribution of word sizes\n",
    "I think the distribution of words by word length is approximately Poisson distributed. I would like to confirm that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size_counts = Counter([len(word) for word in vocabulary2])\n",
    "mean_size = sum([l*size_counts[l] for l in size_counts])/len(vocabulary2)\n",
    "mean_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = mean_size\n",
    "M = len(vocabulary2)\n",
    "bins = max(list(size_counts.keys()))+ 1\n",
    "pdf = np.zeros(bins)\n",
    "pdf[0] = np.exp(-l)\n",
    "for t in range(1,bins):\n",
    "    pdf[t] = pdf[t-1]*l/t\n",
    "    \n",
    "expected = pdf*M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sizes = np.arange(bins-1)\n",
    "counts = np.array([size_counts[l] for l in sizes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title(\"Frequency of word sizes vs. Poisson distribution\")\n",
    "plot(sizes,counts,label='observed');\n",
    "plot(sizes,expected[sizes],label='Poisson distribution');\n",
    "legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
